# 第一章 处理器体系结构

## 本章思考题

**请简述精简指令集 RISC 和复杂指令集 CISC 的区别。**

 RISC只保留了常用的简单指令，risc指令通常在一个cycle内完成。这样处理器不需要浪费太多晶体管去做那些复杂又很少使用的复杂指令。cisc的电路更加复杂。RISC处理器的控制逻辑相对较简单，且容易实现流水线和乱序执行等高级优化技术。



**请简述数值 0x12345678 在大小端字节序处理器的存储器中的存储方式。**

大端存储：低字节放高地址    小端存储：低字节放低地址



**请简述在你所熟悉的处理器（比如双核 Cortex-A9）中一条存储读写指令的执行全过程。**

取指->译码->发射(指令分发)->执行->写回



**请简述内存屏障（memory barrier）产生的原因。**

由于乱序执行，程序对内存的访问顺序可能与程序代码的顺序不一致，会导致内存的乱序访问。从而引发内存不一致的问题。

内存乱序访问主要发生在以下两阶段：

（1）编译时，编译器优化导致内存乱序访问。 （2）运行时，多 CPU 间交互引起的内存乱序访问。



**ARM 有几条 memory barrier 的指令？分别有什么区别？**

有三种内存屏障指令。

（1）数据存储屏障（Data Memory Barrier，DMB） 数据存储器隔离。DMB 指令保证：仅当所有在它前面的存储器访问操作都执行完毕后， 才提交（commit）在它后面的存取访问操作指令。当位于此指令前的所有内存访问均完成 时，DMB 指令才会完成。

（2）数据同步屏障（Data synchronization Barrier，DSB） 数据同步隔离。比 DMB 要严格一些，仅当所有在它前面的存储访问操作指令都执行完 毕后，才会执行在它后面的指令，即任何指令都要等待 DSB 前面的存储访问完成。位于此 指令前的所有缓存，如分支预测和 TLB（Translation Look-aside Buffer）维护操作全部完成。

（3）指令同步屏障（Instruction synchronization Barrier，ISB） 指令同步隔离。它最严格，冲洗流水线（Flush Pipeline）和 预 取 buffers（pretcLbuffers） 后，才会从 cache 或者内存中预取 ISB 指令之后的指令。ISB 通常用来保证上下文切换的效 果，例如更改 ASID（Address Space Identifier）、TLB 维护操作和 C15 寄存器的修改等。



在**x86**架构下，存在两种屏障：

一种是优化屏障。即禁止编译器对指令进行优化重排序。效果就是，在优化屏障之前的指令不可能在优化屏障后的指令之后执行。

另一种是内存屏障。它保证所有在设置读写屏障之前发起的内存访问，必须先于在设置屏障之后发起的内存访问之前完成，确保内存访问按程序的顺序完成。有点像ARM中的数据存储屏障。



**请简述 cache 的工作方式。**

![image-20230625233150723](C:\Users\22816\AppData\Roaming\Typora\typora-user-images\image-20230625233150723.png)

处理器在访问存储器时，会把地址同时传递给 TLB（Translation Lookaside Buffer）和 cache。TLB 是一个用于存储虚拟地址到物理地址转换的小缓存，处理器先使用 EPN （effective page number）在 TLB 中进行查找最终的 RPN（Real Page Number）。如果这期间 发生 TLB miss，将会带来一系列严重的系统惩罚，处理器需要查询页表。假设这里 TLB Hit， 此时很快获得合适的 RPN，并得到相应的物理地址（Physical Address，PA）。 同时，处理器通过 cache 编码地址的索引域（Cache Line Index）可以很快找到相应的 cache line 组。但是这里的 cache block 的数据不一定是处理器所需要的，因此有必要进行一些检查， 将 cache line 中存放的地址和通过虚实地址转换得到的物理地址进行比较。如果相同并且状 态位匹配，那么就会发生 cache 命中（Cache Hit），那么处理器经过字节选择和偏移（Byte Select and Align）部件，最终就可以获取所需要的数据。如果发生 cache miss，处理器需要用物理 地址进一步访问主存储器来获得最终数据，数据也会填充到相应的 cache line 中。



**cache 的映射方式有 full-associative（全关联）、direct-mapping（直接映射）和 set-associative（组相联）3 种方式，请简述它们之间的区别。为什么现代的处理器都使用组 相联的 cache 映射方式？**

减少cache trashing



**在一个 32KB 的 4 路组相联的 cache 中，其中 cache line 为 32Byte，请画出这个 cache 的 cache line、way 和 set 的示意图。**

不画了。way的意思是根据index能够索引到N个way(N路组相连嘛)，其中每一个cacheline是为一个way。set的意思是：比如一个四路组相连，实际上是把整个cache分成了四个set，每个set包含了所有index能够索引到的way。

![image-20230626231413603](C:\Users\22816\AppData\Roaming\Typora\typora-user-images\image-20230626231413603.png)



**ARM9 处理器的 Data Cache 组织方式使用的 VIVT，即虚拟 Index 虚拟 Tag，而在 Cortex-A7 处理器中使用 PIPT，即物理 Index 物理 Tag，请简述 PIPT 比 VIVT 有什么优势？**

VIVT即使用虚拟地址索引域和虚拟地址的标记域。PIPT（Physical Index Physical Tag）：使用物理地址索引域和物理地址的标记域。

采用 VIVT 的方式，不用经过 MMU 的翻译，直接使用虚拟地址的索引域和标记域来查找 cache line，这种方式会导致高速缓存 别名（cache alias）问题。例如一个物理地址的内容可以出现在多个 cache line 中，当系统 改变了虚拟地址到物理地址映射时，需要清洗（clean）和无效（invalidate）这些 cache，导 致系统性能下降。

VIPT 方式，即处理器输出的虚拟地址同时会发送到 TLB/MMU 单元进行地址翻译，以及在 cache 中进行索引和查询 cache 组。这样 cache 和 TLB/MMU 可 以同时工作，当 TLB/MMU 完成地址翻译后，再用物理标记域来匹配 cache line。采用 VIPT 方式的好处之一是在多任务操作系统中，修改了虚拟地址到物理地址映射关系，不需要把 相应的 cache 进行无效（invalidate）操作。采用 VIPT 方式也有可能导致高速缓存别名的问题。在 VIPT 中，使用虚拟地址的索引域 来查找 cache 组，这时有可能导致多个 cache 组映射到同一个物理地址上。

对于 PIPT 方式，索引 域和标记域都采用物理地址，cache 中只有一个 cache 组与之对应，不会产生高速缓存别名 的问题。PIPT 的方式在芯片设计里的逻辑比 VIPT 要复杂得多。



**请画出在二级页表架构中虚拟地址到物理地址查询页表的过程。**

![image-20230626232543696](C:\Users\22816\AppData\Roaming\Typora\typora-user-images\image-20230626232543696.png)



**在多核处理器中，cache 的一致性是如何实现的？请简述 MESI 协议的含义。**

cache一致性协议一般有两种实现方式，一种是软件实现一种是硬件实现。MESI协议是一种总线监听协议，每个处理器的cache通过监听总线实现状态机的转换，从而维护cache一致性，状态机有M\E\S\I四种状态，分别表示cacheline的不同状态：修改态（Modified）、独占态（Exclusive）、共享态（Shared）和失 效态（Invalid）这 4 个状态。cache line 中的状态必须是上述 4 种状态中的一种。至于各个状态之间的转换关系，请看书P22。



**cache 在 Linux 内核中有哪些应用？**

（1）内核中常用的数据结构通常是和 L1 cache 对齐的。例如，mm_struct、fs_cache 等数据 结构使用“SLAB_HWCACHE_ALIGN”标志位来创建 slab 缓存描述符，见 proc_caches_ init() 函数。

（2）一些常用的数据结构在定义时就约定数据结构以 L1 Cache 对齐，使用“____ cacheline_internodealigned_in_smp”和“____cacheline_aligned_in_smp”等宏来定义数据结 构，例如 struct zone、struct irqaction、softirq_vec[ ]、irq_stat[ ]、struct worker_pool 等。

（3）数据结构中频繁访问的成员可以单独占用一个 cache line，或者相关的成员在 cache line 中彼此错开，以提高访问效率。例如，struct zone 数据结构中 zone->lock 和 zone-> lru_lock 这两个频繁被访问的锁，可以让它们各自使用不同的 cache line，以提高获取锁的 效率。

（4）slab 的着色区，见第 2.5 节。 

（5）自旋锁的实现。在多 CPU 系统中，自旋锁的激烈争用过程导致严重的 CPU cacheline bouncing 现象，见第 4 章关于自旋锁的部分内容。



**请简述 ARM big.LITTLE 架构，包括总线连接和 cache 管理等。？**

ARM 提出大小核概念，即 big.LITTLE 架构，针对性能优化过的处理器内核称为大核， 针对低功耗待机优化过的处理器内核称为小核.在典型 big.LITTLE 架构中包含了一个由大核组成的集群（Cortex-A57） 和小核（Cortex-A53）组成的集群，每个集群都属于传统的同步频率架构，工作在相同的 频率和电压下。大核为高性能核心，工作在较高的电压和频率下，消耗更多的能耗，适用 于计算繁重的任务。常见的大核处理器有 Cortex-A15 、 Cortex-A57 、 Cortex-A72 和 Cortex-A73。小核性能虽然较低，但功耗比较低，在一些计算负载不大的任务中，不用开 启大核，直接用小核即可，常见的小核处理器有 Cortex-A7 和 Cortex-A53。

cpu簇连接到CCI-400缓存一致性控制器上，CCI400维护和管理各个cache之间的一致性。

通过 NIC-400 总线来连接一些外设，例如 DMA 设备和 LCD 等。

![image-20230627234327131](C:\Users\22816\AppData\Roaming\Typora\typora-user-images\image-20230627234327131.png)



**cache coherency 和 memory consistency 有什么区别？**

cache coherency 高速缓存一致性关注的是同一个数据在多个 cache 和内存中的一致性 问题，解决高速缓存一致性的方法主要是总线监听协议，例如 MESI 协议等。而 memory consistency 关注的是处理器系统对多个地址进行存储器访问序列的正确性，学术上对内存 访问模型提出了很多，例如严格一致性内存模型、处理器一致性内存模型，以及弱一致性 内存模型等。



**请简述 cache 的 write back 有哪些策略。**

​	一条存储器读写指令经过取指、译码、发射和执行等一系列操作之后， 率先到达 LSU 部件。LSU 部件包括 Load Queue 和 Store Queue，是指令流水线的一个执行部件， 是处理器存储子系统的最顶层，连接指令流水线和 cache 的一个支点。存储器读写指令通过 LSU 之后，会到达 L1 cache 控制器。L1 cache 控制器首先发起探测（Probe）操作，对于读操作发 起 cache 读探测操作并将带回数据，写操作发起 cache 写探测操作。写探测操作之前需要准备 好待写的 cache line，探测工作返回时将会带回数据。当存储器写指令获得最终数据并进行提 交操作之后才会将数据写入，这个写入可以 Write Through 或者 Write Back。 对于写操作，在上述的探测过程中，如果没有找到相应的 cache block，那么就是 Write Miss，否则就是 Write Hit。对于 Write Miss 的处理策略是 Write-Allocate，即 L1 cache 控制 器将分配一个新的 cache line，之后和获取的数据进行合并，然后写入 L1 cache 中。

 Write Through（直写模式）：进行写操作时，数据同时写入当前的 cache、下一级 cache 或主存储器中。Write Through 策略可以降低 cache 一致性的实现难度，其最 大的缺点是消耗比较多的总线带宽。

  Write Back（回写模式）：在进行写操作时，数据直接写入当前 cache，而不会继续 传递，当该 Cache Line 被替换出去时，被改写的数据才会更新到下一级 cache 或主 存储器中。该策略增加了 cache 一致性的实现难度，但是有效降低了总线带宽需求。



**请简述 cache line 的替换策略。**

 随机法：随机地确定替换的 cache block，由一个随机数产生器来生成随机数确定 替换块，这种方法简单，易于实现，但命中率比较低。

  先进先出法：选择最先调入的那个 cache block 进行替换，最先调入的块有可能被 多次命中，但是被优先替换，因而不符合局部性规律。

  最近最少使用算法：LRU 算法根据各块使用的情况，总是选择最近最少使用的块 来替换，这种算法较好地反映了程序局部性规律。



**多进程间频繁切换对 TLB 有什么影响？现代的处理器是如何面对这个问题的？**

TLB shutdown，导致程序刚开始运行时需要多次查表操作，执行速度减缓。现代处理器可以使用大页面，这样TLB可以维护更大区域的内存。



**请简述 NUMA 架构的特点。**

![image-20230628000503740](C:\Users\22816\AppData\Roaming\Typora\typora-user-images\image-20230628000503740.png)



ARM 从 Cortex 系列开始性能有了质的飞越，比如 Cortex-A8/A15/A53/A72，请说 说 Cortex 系列在芯片设计方面做了哪些重大改进？